# Динамические параметры генерации для моделей

## Обзор

Реализована система автоматической настройки параметров генерации в зависимости от выбранной модели Ollama. Каждая модель имеет свои оптимальные диапазоны и рекомендуемые значения параметров.

---

## Основные возможности

### 1. Автоматическая настройка параметров

При выборе модели автоматически устанавливаются:
- ✅ Оптимальная температура
- ✅ Рекомендуемое количество токенов
- ✅ Подходящий размер контекста
- ✅ Длина контекста документа

### 2. Динамические диапазоны

Каждый параметр имеет диапазон, специфичный для модели:
- **Temperature:** от 0 до 1 (или 0.8 для моделей кода)
- **Максимум токенов:** от 50 до 4000 (зависит от размера модели)
- **Размер контекста:** от 512 до 32768 (зависит от возможностей модели)
- **Длина контекста документа:** от 100 до 3000 (зависит от модели)

### 3. Информация о модели

Отображается:
- 📝 Полное название модели
- 🏷️ Категория (быстрая, сбалансированная, мощная, для кода)
- 📄 Описание и рекомендации по использованию

---

## Конфигурация моделей

### Категории моделей

#### ⚡ Быстрые модели (1-2 GB)
**Примеры:** llama3.2:1b, gemma2:2b

**Параметры:**
- Temperature: 0-1 (по умолчанию 0.3)
- Токены: 50-400 (по умолчанию 100-120)
- Контекст: 512-4096 (по умолчанию 1024-2048)
- Длина документа: 100-600 (по умолчанию 200-250)

**Использование:** Простые задачи, быстрые ответы

---

#### ⚖️ Сбалансированные модели (2-5 GB)
**Примеры:** llama3.2:3b, phi3:mini, mistral:7b

**Параметры:**
- Temperature: 0-1 (по умолчанию 0.4-0.6)
- Токены: 50-1000 (по умолчанию 150-256)
- Контекст: 1024-8192 (по умолчанию 2048-4096)
- Длина документа: 150-1000 (по умолчанию 300-400)

**Использование:** Универсальные задачи, оптимальный баланс

---

#### 💪 Мощные модели (5-40 GB)
**Примеры:** llama3.1:8b, gemma2:9b, llama3.1:70b

**Параметры:**
- Temperature: 0-1 (по умолчанию 0.6-0.8)
- Токены: 100-4000 (по умолчанию 400-1024)
- Контекст: 2048-32768 (по умолчанию 4096-8192)
- Длина документа: 300-3000 (по умолчанию 500-1000)

**Использование:** Сложные задачи, детальные ответы

---

#### 💻 Модели для кода
**Примеры:** codellama:7b, deepseek-coder:6.7b

**Параметры:**
- Temperature: 0-0.8 (по умолчанию 0.2) - меньше креативности
- Токены: 100-2000 (по умолчанию 500)
- Контекст: 2048-16384 (по умолчанию 8192) - больше для кода
- Длина документа: 300-2000 (по умолчанию 600)

**Использование:** Программирование, генерация кода

---

## Технические детали

### Структура конфигурации

```typescript
interface ModelConfig {
  name: string
  displayName: string
  temperature: {
    min: number
    max: number
    default: number
    step: number
  }
  maxTokens: { ... }
  contextSize: { ... }
  contextLength: { ... }
  size: string
  category: 'fast' | 'balanced' | 'powerful' | 'code'
  description: string
}
```

### Пример конфигурации

```typescript
'llama3.2:3b': {
  name: 'llama3.2:3b',
  displayName: 'Llama 3.2 3B',
  temperature: { min: 0, max: 1, default: 0.5, step: 0.1 },
  maxTokens: { min: 50, max: 500, default: 150, step: 10 },
  contextSize: { min: 1024, max: 4096, default: 2048, step: 256 },
  contextLength: { min: 150, max: 800, default: 300, step: 50 },
  size: '2.0 GB',
  category: 'balanced',
  description: 'Оптимальный баланс скорости и качества'
}
```

### Функция получения конфигурации

```typescript
function getModelConfig(modelName: string): ModelConfig {
  // 1. Точное совпадение
  if (MODEL_CONFIGS[modelName]) {
    return MODEL_CONFIGS[modelName]
  }

  // 2. Поиск по базовому имени
  const baseName = modelName.split('-')[0]
  if (MODEL_CONFIGS[baseName]) {
    return MODEL_CONFIGS[baseName]
  }

  // 3. Поиск по префиксу
  for (const key in MODEL_CONFIGS) {
    if (modelName.startsWith(key)) {
      return MODEL_CONFIGS[key]
    }
  }

  // 4. Конфигурация по умолчанию
  return DEFAULT_MODEL_CONFIG
}
```

---

## Пользовательский интерфейс

### Выбор модели

```
┌─────────────────────────────────────────────┐
│ Выбрать активную модель:                    │
│ [llama3.2:3b ▼]                             │
├─────────────────────────────────────────────┤
│ ℹ️ Llama 3.2 3B                             │
│    ⚖️ Сбалансированная - универсальная      │
│    Оптимальный баланс скорости и качества   │
└─────────────────────────────────────────────┘
```

### Параметры генерации

```
┌─────────────────────────────────────────────┐
│ Temperature: 0.5        (0 - 1)             │
│ [━━━━━●━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━] │
│ Креативность ответов (рекомендуется: 0.5)  │
├─────────────────────────────────────────────┤
│ Максимум токенов: 150   (50 - 500)         │
│ [━━━━━━━━━●━━━━━━━━━━━━━━━━━━━━━━━━━━━━━] │
│ Длина ответа (рекомендуется: 150)          │
├─────────────────────────────────────────────┤
│ Размер контекста: 2048  (1024 - 4096)      │
│ [━━━━━━━━━━━━━●━━━━━━━━━━━━━━━━━━━━━━━━━] │
│ Память модели (рекомендуется: 2048)        │
├─────────────────────────────────────────────┤
│ Длина контекста документа: 300 (150 - 800) │
│ [━━━━━━━━━━━●━━━━━━━━━━━━━━━━━━━━━━━━━━━] │
│ Применяется ко всем моделям (рек.: 300)    │
└─────────────────────────────────────────────┘
```

---

## Сценарии использования

### Сценарий 1: Смена модели

1. Пользователь выбирает **llama3.2:1b** (быстрая модель)
2. Автоматически устанавливаются:
   - Temperature: 0.3
   - Токены: 100
   - Контекст: 1024
   - Длина документа: 200
3. Диапазоны слайдеров обновляются:
   - Temperature: 0-1
   - Токены: 50-300
   - Контекст: 512-2048
   - Длина документа: 100-500
4. Показывается уведомление: "✓ Параметры настроены для Llama 3.2 1B"

### Сценарий 2: Переход на мощную модель

1. Пользователь выбирает **llama3.1:8b** (мощная модель)
2. Автоматически устанавливаются:
   - Temperature: 0.7
   - Токены: 512
   - Контекст: 4096
   - Длина документа: 500
3. Диапазоны расширяются:
   - Токены: 100-2000
   - Контекст: 2048-8192
   - Длина документа: 300-1500

### Сценарий 3: Модель для кода

1. Пользователь выбирает **codellama:7b**
2. Автоматически устанавливаются:
   - Temperature: 0.2 (меньше креативности)
   - Токены: 500
   - Контекст: 8192 (больше для кода)
   - Длина документа: 600
3. Максимальная температура ограничена 0.8

---

## Преимущества

### Для пользователя:
- ✅ Не нужно знать оптимальные параметры
- ✅ Автоматическая настройка при смене модели
- ✅ Видны рекомендуемые значения
- ✅ Понятные диапазоны для каждой модели
- ✅ Информация о категории и назначении модели

### Для системы:
- ✅ Оптимальное использование ресурсов
- ✅ Предотвращение некорректных настроек
- ✅ Улучшенное качество ответов
- ✅ Адаптация под возможности модели

---

## Файлы

### Новые файлы:
1. **frontend/src/config/modelConfigs.ts**
   - Конфигурации 12 популярных моделей
   - Функции получения конфигурации
   - Описания категорий

### Обновленные файлы:
2. **frontend/src/components/tabs/SettingsTab.tsx**
   - Импорт конфигураций
   - Функция `handleModelChange()`
   - Динамические диапазоны слайдеров
   - Информационный блок о модели
   - Рекомендуемые значения

---

## Расширение

### Добавление новой модели

```typescript
// В modelConfigs.ts
'new-model:7b': {
  name: 'new-model:7b',
  displayName: 'New Model 7B',
  temperature: { min: 0, max: 1, default: 0.5, step: 0.1 },
  maxTokens: { min: 100, max: 1000, default: 300, step: 20 },
  contextSize: { min: 2048, max: 8192, default: 4096, step: 512 },
  contextLength: { min: 200, max: 1000, default: 400, step: 50 },
  size: '4.0 GB',
  category: 'balanced',
  description: 'Описание новой модели'
}
```

### Добавление новой категории

```typescript
// Добавить в тип
category: 'fast' | 'balanced' | 'powerful' | 'code' | 'multimodal'

// Добавить описание
function getCategoryDescription(category: ModelConfig['category']): string {
  const descriptions = {
    // ...
    multimodal: '🖼️ Мультимодальная - текст и изображения'
  }
  return descriptions[category]
}
```

---

## Будущие улучшения

### Возможные доработки:
- 📊 Визуализация использования ресурсов
- 🎯 Пресеты для разных задач (чат, анализ, код)
- 📈 История изменений параметров
- 💾 Сохранение профилей настроек
- 🔄 Синхронизация с Ollama API для получения реальных возможностей
- 🧪 A/B тестирование параметров
- 📱 Адаптивные параметры для мобильных устройств
- 🔍 Поиск оптимальных параметров

---

## Тестирование

### Проверка функциональности:

1. **Смена модели:**
   ```
   Выбрать llama3.2:1b → Параметры обновляются
   Выбрать llama3.1:8b → Диапазоны расширяются
   ```

2. **Информация о модели:**
   ```
   Выбрать модель → Показывается описание
   Проверить категорию → Соответствует типу
   ```

3. **Диапазоны параметров:**
   ```
   Быстрая модель → Меньшие диапазоны
   Мощная модель → Большие диапазоны
   Модель для кода → Temperature max 0.8
   ```

4. **Рекомендуемые значения:**
   ```
   Проверить подсказки → Показаны рекомендации
   Сбросить → Устанавливаются рекомендуемые
   ```

---

## Заключение

Система динамических параметров значительно упрощает настройку моделей и обеспечивает оптимальную работу каждой модели. Пользователи получают автоматические рекомендации, а система предотвращает некорректные настройки.
